{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "music_generation_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFiEF_9x1QRs"
      },
      "source": [
        "!gdown --id 1EI0V4JRkAMhjwayaDIRC57iq8liHEiCS\n",
        "!unzip chopin.zip\n",
        "#!rm -r mid_data\n",
        "!mkdir mid_data\n",
        "!mkdir data\n",
        "!mkdir mid_out\n",
        "!cp -r /content/*.mid /content/mid_data\n",
        "!rm /content/*.mid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm8ICPKp1nqv"
      },
      "source": [
        "import glob\n",
        "import pickle\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "from music21 import converter, instrument, note, chord, stream\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dropout, Dense, Activation, BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBOr_dke1fEe"
      },
      "source": [
        "def mid_to_list(file_name):\n",
        "   # return notes and cordes from .mid file as a list\n",
        "   # NB: MIDI files are specific music files extention contain info about instruction to create the \n",
        "   # music (like a music sheet) \n",
        "\n",
        "  musical_elements = [] \n",
        "  for file in glob.glob(file_name):\n",
        "      midi = converter.parse(file)\n",
        "      notes_to_parse = None\n",
        "      parts = instrument.partitionByInstrument(midi)\n",
        "      if parts: # file has instrument parts\n",
        "          notes_to_parse = parts.parts[0].recurse()\n",
        "      else: # file has notes in a flat structure\n",
        "          notes_to_parse = midi.flat.notes\n",
        "      for element in notes_to_parse:\n",
        "          if isinstance(element, note.Note):\n",
        "              musical_elements.append(str(element.pitch))\n",
        "          elif isinstance(element, chord.Chord):\n",
        "              musical_elements.append('.'.join(str(n) for n in element.normalOrder))\n",
        "  with open('data/musical_elements', 'wb') as filepath:\n",
        "    pickle.dump(musical_elements, filepath)\n",
        "  return musical_elements \n",
        " \n",
        "def list_to_midi(prediction_output, file_name):\n",
        "    # return .mid file from list of music elements\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "\n",
        "    # create note and chord objects based on the values generated by the model\n",
        "    for pattern in prediction_output:\n",
        "        # pattern is a chord\n",
        "        if ('.' in pattern) or pattern.isdigit():\n",
        "            notes_in_chord = pattern.split('.')\n",
        "            notes = []\n",
        "            for current_note in notes_in_chord:\n",
        "                new_note = note.Note(int(current_note))\n",
        "                new_note.storedInstrument = instrument.Piano()\n",
        "                notes.append(new_note)\n",
        "            new_chord = chord.Chord(notes)\n",
        "            new_chord.offset = offset\n",
        "            output_notes.append(new_chord)\n",
        "        # pattern is a note\n",
        "        else:\n",
        "            new_note = note.Note(pattern)\n",
        "            new_note.offset = offset\n",
        "            new_note.storedInstrument = instrument.Piano()\n",
        "            output_notes.append(new_note)\n",
        "\n",
        "        # increase offset each iteration so that notes do not stack\n",
        "        offset += 0.5\n",
        "\n",
        "    midi_stream = stream.Stream(output_notes)\n",
        "\n",
        "    midi_stream.write('midi', fp=file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHAyP1vM9sDP"
      },
      "source": [
        "# #test encoder and decoder\n",
        "# res = mid_to_list(\"mid_data/chp_op18_format0.mid\")\n",
        "# list_to_midi(res, 'mid_out/test.mid')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLPeHSzoBP4l"
      },
      "source": [
        "def pre_process_data(musical_elts, n_vocab, seq_len=100, stride=1):\n",
        "    #preprocess musical elements and return data as matrix of LSTM inputs and outputs\n",
        "    dataX = []\n",
        "    dataY = []\n",
        "\n",
        "    # create input sequences and the corresponding outputs\n",
        "    for i in range(0, len(musical_elts) - seq_len, stride):\n",
        "        seq_in = musical_elts[i:i + seq_len]\n",
        "        seq_out = musical_elts[i + seq_len]\n",
        "        dataX.append([music_el_to_int[el] for el in seq_in])\n",
        "        dataY.append(music_el_to_int[seq_out])\n",
        "\n",
        "    n_patterns = len(dataX)\n",
        "    print(\"Total examples synthesised from this dataset: \", n_patterns)\n",
        "\n",
        "    # reshape the input into a format compatible with LSTM layers\n",
        "    normalized_X = numpy.reshape(dataX, (n_patterns, seq_len, 1))\n",
        "    # normalize input\n",
        "    normalized_X = normalized_X / float(n_vocab)\n",
        "\n",
        "    categorical_Y = np_utils.to_categorical(dataY)\n",
        "\n",
        "    return normalized_X, categorical_Y, dataX "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1j3flsk4WWl"
      },
      "source": [
        "musical_elements = mid_to_list(\"mid_data/*.mid\")\n",
        "n_vocab = len(set(musical_elements))\n",
        "print(\"Total Vocab\", n_vocab)\n",
        "print(\"vocabulary is the number of distinct values (notes, cordes) used in the midi file to make the song\")\n",
        "\n",
        "# get sorted set of music elements\n",
        "sorted_set_of_elements = sorted(set(musical_elements))\n",
        "\n",
        "# map : music_element to int\n",
        "music_el_to_int = dict((musical_el, i) for i, musical_el in enumerate(sorted_set_of_elements))\n",
        "int_to_music_el = dict((i, musical_el) for i, musical_el in enumerate(sorted_set_of_elements))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV0c_a8nUWvh"
      },
      "source": [
        "normalized_X, categorical_Y, dataX  = pre_process_data(musical_elements, n_vocab)\n",
        "print(\"shape of X\", normalized_X.shape)\n",
        "print(\"shape of Y\", categorical_Y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6msBKnZLB78r"
      },
      "source": [
        "#creating the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(512, return_sequences=True, input_shape=(normalized_X.shape[1], normalized_X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(512, return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(n_vocab))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(\n",
        "#     LSTM(512,input_shape=(dataX.shape[1], dataX.shape[2]), recurrent_dropout=0.3,return_sequences=True)\n",
        "#     )\n",
        "# model.add(LSTM(512, return_sequences=True, recurrent_dropout=0.3,))\n",
        "# model.add(LSTM(512))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.3))\n",
        "# model.add(Dense(256))\n",
        "# model.add(Activation('relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.3))\n",
        "# model.add(Dense(n_vocab))\n",
        "# model.add(Activation('softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQY0Mab9LeKv"
      },
      "source": [
        "#we compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjBOQlETLq3J"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuWRCVBkG5B3"
      },
      "source": [
        "#set up checkpoints config\n",
        "cpfilepath = \"v1-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(cpfilepath, monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f72R22P8S11j"
      },
      "source": [
        "#load pretrained model\n",
        "filename = \"pre_trained_weights.hdf5\"\n",
        "model.load_weights(filename)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukCheg4MLwjr"
      },
      "source": [
        "#training\n",
        "history = model.fit(normalized_X, categorical_Y, epochs=50, batch_size=128, callbacks=callbacks_list)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHmiG0ddMx9A"
      },
      "source": [
        "#music generation\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "\n",
        "pattern = dataX[start]\n",
        "predicted_out = []\n",
        "\n",
        "#start generation\n",
        "for note_index in range(500):\n",
        "  x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "  x = x / float(n_vocab)\n",
        "  prediction = model.predict(x, verbose=0)\n",
        "  index = numpy.argmax(prediction)\n",
        "  result = int_to_music_el[index]\n",
        "  predicted_out.append(result)\n",
        "  pattern.append(index)\n",
        "  pattern = pattern[1:len(pattern)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbzeLE3lQDSF"
      },
      "source": [
        "#create MIDI music file\n",
        "list_to_midi(predicted_out, 'mid_out/test.mid')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAMM3zHuSsBp"
      },
      "source": [
        "#utils\n",
        "#!zip -r weights_to_zip v1-*\n",
        "#files.download(\"weights_to_zip.zip\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p_8qYNTW9jn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VPHx2TVfmBF"
      },
      "source": [
        "  #music analysis Lab :)\n",
        "   \n",
        "  musical_elements = [] \n",
        "  raw_elements     = [] \n",
        "  for file in glob.glob(\"mid_data/chp_op18_format0.mid\"):\n",
        "      midi = converter.parse(file)  \n",
        "      notes_to_parse = None\n",
        "      parts = instrument.partitionByInstrument(midi)\n",
        "      if parts: # file has instrument parts\n",
        "          notes_to_parse = parts.parts[0].recurse()\n",
        "      else: # file has notes in a flat structure\n",
        "          notes_to_parse = midi.flat.notes\n",
        "      for element in notes_to_parse:\n",
        "          raw_elements.append(element)\n",
        "          if isinstance(element, note.Note):\n",
        "              musical_elements.append(str(element.pitch))\n",
        "          elif isinstance(element, chord.Chord):\n",
        "              musical_elements.append('.'.join(str(n) for n in element.normalOrder))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrGOW3EB7RaQ"
      },
      "source": [
        "from music21 import key\n",
        "\n",
        "raw_elements\n",
        "str_elts = []\n",
        "for elm in raw_elements:\n",
        "  print(elm)\n",
        "  # if isinstance(elm, note.Note):\n",
        "  #   print(str(elm))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}